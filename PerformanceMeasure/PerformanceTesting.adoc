= MidPoint Performance Benchmarking Plan

== 1 Objectives & Scope

[cols="1,3,2",options="header"]
|===
|Level | Goal | In / Out of Scope
|Primary | Quantify runtime and throughput of *Import → Live Sync → Reconciliation* under controlled load. | In: Core operations on single-node midPoint
|Secondary | Identify resource bottlenecks (CPU, RAM, DB-I/O) to support capacity planning. | Out: UI performance, approvals, HA setups
|===

== 2 Test Variables vs Constants

[cols="1,2,2",options="header"]
|===
|Category | Independent Variables (change) | Constants (freeze)
|Resource Count | 5, 10, 20, 30, 50 | VM hardware spec (CPU/RAM/Disk)
|Assignments per User | 1, 5, 10 | midPoint version, PostgreSQL, OpenJDK
|Mapping Complexity | Simple (1-to-1) ↔ Complex (Groovy, conditions) | 1000-user HR dataset (CSV)
|User Delta Size | 1%, 10%, 100% for Live Sync | Fixed HR schema
|Connector Latency (optional) | 0ms, 100ms emulated (tc/netem) | Host-only local network
|===

== 3 KPIs & Instrumentation

[cols="1,2,2",options="header"]
|===
|KPI | Tool / Method | Notes
|Task Duration | Audit logs, task view | Record start/end timestamps
|Throughput (ops/sec) | Derived from duration | Total records / duration
|CPU Load % | `vmstat`, `sar`, Prometheus | Track midPoint + DB
|Heap Usage | JMX, VisualVM, Prometheus exporter | Peak usage post-GC
|DB TPS / Queries | `pg_stat_statements` | Measure slow queries
|Connector Latency | midPoint perf counters | Optional, if available
|===

== 4 Test Matrix Template

[cols="^1,^1,^1,^1,^2,^2,^2,^1,^1",options="header"]
|===
|ID | # Resources | Assign/User | Mapping | Import (s) | Sync (s) | Recon (s) | CPU % | Heap GB
|BASE-01 | 1 | 1 | Simple | - | - | - | - | -
|T-05-1 | 5 | 1 | Simple | | | | |
|T-10-5 | 10 | 5 | Simple | | | | |
|T-20-5C | 20 | 5 | Complex | | | | |
|T-30-10 | 30 | 10 | Simple | | | | |
|T-50-10C | 50 | 10 | Complex | | | | |
|===

Repeat each test 3 times for average and standard deviation.

== 5 Execution Playbook

This section assumes midPoint is already installed and operational. The focus is strictly on performance benchmarking activities.

=== 5.1 Test Data Generation

==== Generate Realistic Users

Install the Faker library:

[source,bash]
----
pip3 install faker
----

Save the following as `gen_users_realistic.py`:

[source,python]
----
from faker import Faker
import csv

fake = Faker()
Faker.seed(0)

users = []
for i in range(1, 1001):
first = fake.first_name()
last = fake.last_name()
email = f"{first.lower()}.{last.lower()}{i}@example.com"
users.append({
'employeeId': f'E{i:04d}',
'firstName': first,
'lastName': last,
'email': email,
'status': 'active'
})

with open('users.csv', 'w', newline='') as f:
writer = csv.DictWriter(f, fieldnames=users[0].keys())
writer.writeheader()
writer.writerows(users)
----

Run it:

[source,bash]
----
python3 gen_users_realistic.py
----

==== Optional: Expand to 10k+ Users

To scale up:

[source,python]
----
for i in range(1, 10001):  # 10,000 users
...
----

=== 5.2 Resource & Role Preparation

==== Create Roles & Assignments

Design a template role with inducements:

[source,xml]
----
<role>
  <name>App-X-User</name>
  <inducement>
    <construction>
      <resourceRef oid="..." type="ResourceType"/>
    </construction>
  </inducement>
</role>
----

Repeat or vary based on test matrix (e.g. 5 roles assigned per user).

==== Automate Resource Creation

Create `generate_resources.sh`:

[source,bash]
----
#!/bin/bash
for i in $(seq 1 10); do
cp resource-template.xml resource-${i}.xml
sed -i "s/**RESOURCE_ID**/resource-${i}/g" resource-${i}.xml
sed -i "s/**PORT**/$((8080 + $i))/g" resource-${i}.xml
mpcli import resource-${i}.xml
done
----

Make it executable:

[source,bash]
----
chmod +x generate_resources.sh
./generate_resources.sh
----

=== 5.3 Import Users into midPoint

Transform `users.csv` to XML using custom script or upload via GUI:

. Import using UI:

* Resources → HR CSV → Import
  . Or use REST:

[source,bash]
----
curl -u administrator:5ecr3t 
-H "Content-Type: text/csv" 
--data-binary @users.csv 
[http://localhost:8080/midpoint/ws/rest/users/import](http://localhost:8080/midpoint/ws/rest/users/import)
----

=== 5.4 Monitoring Setup

Install tools:

[source,bash]
----
sudo apt install -y htop iotop sysstat
----

Start resource tracking:

[source,bash]
----
vmstat 1 > vmstat_test01.log &
----

=== 5.5 Run Benchmarks

For each test case (row in matrix):

. Reset users/assignments/resources if needed
. Import users
. Assign roles
. Trigger task (Import / Live Sync / Reconciliation)
. Monitor task via GUI or REST
. Record:

* Start timestamp
* End timestamp
* CPU usage (from `vmstat`)
* Heap usage (from JMX/VisualVM)
  . Save audit logs:

[source,bash]
----
cp /opt/midpoint/var/log/audit.log results/audit_${ID}.log
cp vmstat_test01.log results/vmstat_${ID}.log
----

Repeat each test 3 times.

* Use Pandas / Excel to aggregate:
  ** Average duration
  ** Standard deviation
  ** Peak usage stats

* Visualizations:
  ** Line graph: Resources vs Duration
  ** Bar chart: Peak Heap Usage
  ** Heatmap: CPU by Test ID

* Analysis Questions:
  ** When does performance become non-linear?
  ** Is heap pressure reaching GC thresholds?
  ** Is PostgreSQL maxing out before midPoint?

== 7 Scaling & Follow-up Tests

* Increase dataset to 10k / 100k users
* HA benchmark: repeat matrix in 2-node clustered midPoint
* Use live connectors (e.g., Azure AD, Okta) for latency realism
* Schedule test automation via GitHub Actions on Hyper-V host

== 8 Optional Assets (on request)

* Shell/PowerShell setup scripts for VM bootstrap
* Docker Compose file for 50 mock SCIM endpoints
* `gen_users.py` and CSV format examples
* Grafana starter dashboard JSON
* REST benchmarking JMeter plan
* Export data to Excel for graphs
* Build Prometheus + Grafana dashboards
* Optionally upload results to GitHub Wiki or Confluence
